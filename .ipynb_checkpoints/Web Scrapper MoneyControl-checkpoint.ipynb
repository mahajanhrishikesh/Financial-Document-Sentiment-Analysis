{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping notebook for Scraping site www.moneycontrol.com\n",
    "#### Name: Hrishikesh Milind Mahajan\n",
    "#### Roll No.: PC-45\n",
    "#### Serial No.: 1032171054\n",
    "##### Dataset generated for seminar topic: Machine Learning techniques for Financial Sentiment Analysis\n",
    "\n",
    "    Libraries Used: \n",
    "        1. Newspaper (https://github.com/codelucas/newspaper)\n",
    "        2. NLTK (https://github.com/nltk/nltk)\n",
    "        3. Beautiful Soup (https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "        4. URLLIB (https://github.com/urllib3/urllib3)\n",
    "    \n",
    "    About Data:\n",
    "        Data generated is collected from moneycontrol website, the data contains the following fields....\n",
    "        1. Author Name\n",
    "        2. Summary\n",
    "        3. Time of Upload \n",
    "        4. Text inside article\n",
    "        5. Title of Article\n",
    "        6. Metadata Description\n",
    "        7. URL of the article \n",
    "        \n",
    "    Procedure followed:\n",
    "        1. Accept Page number range from user\n",
    "        2. Crawl the HTML page for links inside article list DOM\n",
    "        3. Append each link to URL in list\n",
    "        4. Send list to Newspaper API\n",
    "        5. Fetch articles one by one and fill out above data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'newspaper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-83b806c39519>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnewspaper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'newspaper'"
     ]
    }
   ],
   "source": [
    "import newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rishikesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request \n",
    "request_url = urllib.request.urlopen('https://www.moneycontrol.com/news/business/stocks/') \n",
    "soup = BeautifulSoup(request_url, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Following function fetches the URLs for each article in a single page\n",
    "def get_mc_urls(base_url):\n",
    "    urls_to_parse = []\n",
    "    request_url = urllib.request.urlopen(base_url) \n",
    "    soup = BeautifulSoup(request_url, 'html.parser')\n",
    "    l = soup.find(id = 'cagetory')\n",
    "    refs = l.find_all('a')\n",
    "    nl = l.find_all('li')\n",
    "    print(len(refs), len(nl))\n",
    "    for i in range(len(nl)):\n",
    "        try:\n",
    "            urls_to_parse.append((nl[i].find('a')['href'], nl[i].find('span').text))\n",
    "        except:\n",
    "            pass\n",
    "    return list(set(urls_to_parse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserter gathers data from newspaper article and stores in pandas DataFrame\n",
    "from newspaper import Article\n",
    "import pandas as pd\n",
    "def inserter(urls_to_parse):\n",
    "    authors_list = []\n",
    "    summary_list = []\n",
    "    time_upload = []\n",
    "    article_text_list = []\n",
    "    title_list = []\n",
    "    url_list = []\n",
    "    description_list = []\n",
    "    for url in urls_to_parse:\n",
    "        #print(f'Parsing URL {url[0]}')\n",
    "        article = Article(url[0])\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        article.nlp()\n",
    "        try:\n",
    "            authors_list.append(article.authors[0])\n",
    "        except:\n",
    "            authors_list.append('')\n",
    "        summary_list.append(article.summary)\n",
    "        time_upload.append(url[1])\n",
    "        article_text_list.append(article.text)\n",
    "        title_list.append(article.title)\n",
    "        url_list.append(article.url)\n",
    "        description_list.append(article.meta_description)\n",
    "    return pd.DataFrame({'time':time_upload, 'author': authors_list, 'title': title_list, 'summary' : summary_list, 'description': description_list, 'article_text': article_text_list, 'url':url_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver code\n",
    "backup_main_data = pd.DataFrame()\n",
    "def main_scraper(base_url, p1, pN):\n",
    "    print(f'Scraping {base_url}')\n",
    "    main_data = pd.DataFrame()\n",
    "    start = True\n",
    "    for page_id in range(p1, pN):\n",
    "        print(f'Scrapping page {page_id} of {pN-p1} pages.')\n",
    "        if start:\n",
    "            start = False\n",
    "            urls_to_parse = get_mc_urls(base_url)\n",
    "            temp = inserter(urls_to_parse)\n",
    "            main_data = temp\n",
    "        else:\n",
    "            base_url = base_url+'page-'+str(page_id)+'/'\n",
    "            urls_to_parse = get_mc_urls(base_url)\n",
    "            temp = inserter(urls_to_parse)\n",
    "            main_data = pd.concat([main_data, temp])\n",
    "            backup_main_data = main_data\n",
    "        print(f'Completed scrapping page {page_id}.')\n",
    "    print('Dataset ready!')\n",
    "    return main_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non Combination\n",
    "def page_scraper(base_url):\n",
    "    print(f'Scraping {base_url}')\n",
    "    urls_to_parse = get_mc_urls(base_url)\n",
    "    temp = inserter(urls_to_parse)\n",
    "    print('Dataset ready!')\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://www.moneycontrol.com/news/business/stocks/page-11/\n",
      "50 31\n",
      "Dataset ready!\n",
      "25\n",
      "Scraping https://www.moneycontrol.com/news/business/stocks/page-12/\n",
      "50 31\n",
      "Dataset ready!\n",
      "50\n",
      "Scraping https://www.moneycontrol.com/news/business/stocks/page-13/\n",
      "50 31\n",
      "Dataset ready!\n",
      "75\n",
      "Scraping https://www.moneycontrol.com/news/business/stocks/page-14/\n",
      "50 31\n",
      "Dataset ready!\n",
      "100\n",
      "Scraping https://www.moneycontrol.com/news/business/stocks/page-15/\n",
      "50 31\n",
      "Dataset ready!\n",
      "125\n",
      "Scraping https://www.moneycontrol.com/news/business/stocks/page-16/\n",
      "50 31\n",
      "Dataset ready!\n",
      "150\n",
      "Scraping https://www.moneycontrol.com/news/business/stocks/page-17/\n",
      "50 31\n",
      "Dataset ready!\n",
      "175\n",
      "Scraping https://www.moneycontrol.com/news/business/stocks/page-18/\n",
      "50 31\n",
      "Dataset ready!\n",
      "200\n",
      "Scraping https://www.moneycontrol.com/news/business/stocks/page-19/\n",
      "50 31\n",
      "Dataset ready!\n",
      "225\n",
      "Scraping https://www.moneycontrol.com/news/business/stocks/page-20/\n",
      "50 31\n",
      "Dataset ready!\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "main_data = pd.DataFrame()\n",
    "for i in range(11,21):\n",
    "    page = page_scraper(f'https://www.moneycontrol.com/news/business/stocks/page-{i}/')\n",
    "    main_data = pd.concat([main_data, page])\n",
    "    print(len(main_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data.to_csv('MoneyControl-First-11-20-Pages.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
